# 神经网络入门
## 神经网络的介绍

### 单个神经细胞的模型化

神经元：多个输入---->权重--->偏置--->激励函数--->输出
符号表示：x为输入函数，W为权重，b为偏置常量，f为激励函数，y为输出
则$y=f（\sum（x_i * w_i+b)）$

### 神经元的网络化

在神经网络中，网络层包括`输入层`，`中间层（隐藏层）`，`输出层`。
正向传播：从转递输入信息到产生输出的过程
反向传播：从输出向输入逆向传递信息的过程
符号表示：
前面网络层有m个，后面网络层有n个。则Wij：前面网络层i对后面网络层j的权重，W为m *n的矩阵。
上层网络层有m个神经元，即上层网络的输出向量的元素为m。
i作为上层网络的下标，j作为下层网络的下标。
$Y_i=X_j=(x_1,x_2,x_3...x_m)$ *下层网络层的每一个神经元都有m个输入*
下层神经网络的神经元偏置$B_j=（b_1，b_2,b_3...b_n)$

下层神经网络的输出为$Y_j=(y_1,y_2,y_3...y_n)$

则$Yj=f（\sum(X_j *W_{ij})+B_j)$

### 神经网络的应用

- 回归问题：连续性问题
- 分类问题：离散问题

## 神经网络的架构

### 激活函数

#### 阶跃函数

```python
numpy.where(x<=0,0,1)
```

#### sigmoid函数

```python
numpy.where(1/1+exp(-x))
```

sigmid的导数：$y'=（1-y)*y$

#### tanh函数

numpy内置tanh函数：$tanh（x)=\frac{(exp(x)-exp(-x))}{(exp(x)+exp(-x))}$

```python
numpy.tanh(x)
```

#### ReLU函数

```python
numpy.where(x<=0,0,x)
```

#### Leaky Relu函数

```python
numpy.where(x<=0,0.01*x,x)
```

对于ReLU的优化，对负数区域增加小的梯度避免出现：dying ReLU现象

#### 恒等函数

用于输出回归函数的输出层：y=x

#### SoftMax函数

$y=exp(X)/\sum exp(X_k)$

```python
numpy.exp(x)/numpy.sum(numpy.exp(x))
```

### 损失函数

对输出与正确答案的误差进行定义的函数就是损失函数

#### 平方和误差（回归问题）

$E=\frac{1}{2}\sum(y_k-t_k)^2$

其中 E 表示误差，$y_k$表示输出层的各个输出值，$t_k$表示正确答案

```python
import numpy as np
def square_sum(y,t):
    1.0/2.0*np.sum(np.square(y-t))
```

#### 交叉熵误差(分类问题)

$E=-\sum t_klog(y_k)$

其中 E 表示误差，$t_k$表示正确答案，$y_k$表示输出答案，由于$t_k$是独热编码，故只有唯一正确为1的项对误差产生影响。

```python
import numpy as np
def cross_entropy(y,t):
	return -np.sum(t*np.log(y+1e-7))
```

### 学习法则--参数更新的方法

#### 赫布学习法则

∆$\omega$=$\varphi y_i y_j$ 

其中$\Delta \omega$ 表示为连接强度（权重）的变化量，$\varphi$  为一常量， $y_i$ 表示为突触前膜神经元的兴奋程度,$y_j$ 表示为突触后膜神经元的兴奋程度。

意义：突触前膜和突触后膜产生的兴奋使突触的能量传递效率增强。与之相反，长时间没有兴奋，突触的能量传递效率衰退。

#### Delta学习法则

$\Delta\omega=\eta(y_j-t)y_i$ 

其中$\Delta \omega$ 表示为权重的变化量，$y_i$ 突触的前神经元的输出，$y_j$ 表示突触的后神经元的输出，t表示正确答案，$\eta$表示学习系数的常数。

意义：

- 如果输出与正确答案之间的差值越大，则需要设置的权重的修正量也越大。
- 如果输入越大，则需要设置的权重的修正量也越大。

### 参数更新的方法--最优化算法

- 随机梯度下降法(SGD)
- Momentum
- AdaGrad
- RMSProp
- Adam

####  随机下降梯度法(SGD)

更新公式：
$w\leftarrow w-\eta\frac{∂ E}{∂ w}$

$b\leftarrow b-\eta\frac{∂ E}{∂ b}$

优点：随机选样本，不容易掉入局部最优解。简单确定更新量，简单的代码实现。

缺点：在学习的过程中无法对更新量灵活调整

####  Momentum

$w\leftarrow w-\eta\frac{∂ E}{∂ w}+\alpha\Delta w$

$b\leftarrow b-\eta\frac{∂ E}{∂ b}+\alpha\Delta b$

$\alpha$决定惯性的强度常量，$\Delta w$表示前一次的更新量

优点：防止更新量的急剧变化

缺点：必须事先给定$\eta$和 $\alpha$，增加网络调整的难度。

#### AdaFrad

$ h\leftarrow h+(\frac{∂ E}{∂ w})^2$

$ w\leftarrow w-\eta\frac{1}{\sqrt{h}}\frac{∂ E}{∂ w}$

$ h\leftarrow h+(\frac{∂ E}{∂ b})^2$

$ b\leftarrow b-\eta\frac{1}{\sqrt{h}}\frac{∂ E}{∂ b}$

优点：对更新量进行调整

缺点：更新量持续减少

#### RMSPorp

$ h\leftarrow \rho h+(1-\rho)(\frac{∂ E}{∂ w})^2$

$ w\leftarrow w-\eta\frac{1}{\sqrt{h}}\frac{∂ E}{∂ w}$

$ h\leftarrow \rho h+(1-\rho)(\frac{∂ E}{∂ b})^2$

$ b\leftarrow b-\eta\frac{1}{\sqrt{h}}\frac{∂ E}{∂ b}$

$\rho$一般设置为0.9。

#### Adam

$m_0=v_0=0$

$ m_t=\beta_1m_{t-1}+(1-\beta_1)\frac{∂ E}{∂ w}$

$ v_t=\beta_2v_{t-1}+(1-\beta_2)(\frac{∂ E}{∂ w})^2$

$\hat{m_t}=\frac{m_{0t}}{1-\beta_1^t}$

$\hat{v_t}=\frac{v_t}{1-\beta_2^t}$

$w \leftarrow w-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\omicron}$

$\beta_1=0.9 \ \ \beta_2=0.999\ \ \eta=0.001\ \ \ \omicron=10^{-8}  $

t表示重复次数

