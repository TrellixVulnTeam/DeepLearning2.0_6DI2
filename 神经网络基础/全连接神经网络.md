# 全连接神经网络--MLP

全连接神经网络，又叫多层感知机，是一种连接方式较为简单的人工神经网络，是前馈神经网络的一种。

## MLP的神经网络架构

网络架构为：

![1](D:\深度学习笔记\1.jpg)

通过输入层，隐藏层，输出层三个网络层组成，其中隐藏层可以有多层。

## BP传播的原理

### 梯度下降法

正向传播，对网络层的输出进行传播，反向传播，对输入的梯度进行传播。

输出层：

- 偏置的梯度
- 权重的梯度
- 输入的梯度

中间层：

- 偏置的梯度
- 权重的梯度
- 输入的梯度

| 网络层  | 下表   | 神经元数量 |
| ---- | ---- | :---- |
| 输入层  | i    | l     |
| 中间层  | j    | m     |
| 输出层  | k    | n     |

#### 输出层的梯度

##### 相关函数

$ E=f_1(y_k)$

$y_k=f_2(u_k)$

$u_k=\sum_{q=1}^{m}(y_qw_{qk}+b_k)$

E:损失值

$f_1$:损失函数

$y_k$:输出结果

$f_2$:激励函数

$u_k$:输入与权值乘积与偏置的求和

$y_j$:$y_j$为$y_q$中的一个

##### 定义

$\delta_k=\frac{∂E}{∂u_K}=\frac{\partial E}{\partial y_k}\frac{\partial y_k}{\partial u_k}$

$\frac{\partial E}{\partial y_k}$损失函数对$y_k$的偏微分

$\frac{\partial y_k}{\partial u_k}$激励函数对$u_k$的偏微分

##### 输出层权重的梯度

$\partial w_{jk} = \frac{\partial E}{\partial w_{jk}}$

根据微分连锁公式

$\frac{\partial E}{\partial w_{jk}}=\frac{\partial E}{\partial y_k}\frac{\partial y_k}{\partial u_k}\frac{\partial u_k}{\partial w_{jk}}$

$\frac{\partial u_k}{\partial w_{jk}}=y_j$   :$u_k$对$w_{jk}$的偏微分

$\partial w_{jk} =\delta_ky_j$

##### 输出层偏置的梯度

$\partial b_k = \frac{\partial E}{\partial b_k}$

根据微分连锁公式

$\frac{\partial E}{\partial b_k}=\frac{\partial E}{\partial y_k}\frac{\partial y_k}{\partial u_k}\frac{\partial u_k}{\partial b_k}$

$\frac{\partial u_k}{\partial b_k}=1$  : $u_k$对$b_k$的偏微分

$\partial b_k =\delta_k$

#### 输出层的输入梯度

输出层的输入梯度=中间层的输出梯度

$\partial y_j= \frac{\partial E}{\partial y_j}$

根据微分连锁公式

$\frac{\partial E}{\partial y_j}=\sum_{r=1}^{n}\frac{\partial E}{\partial y_r}\frac{\partial y_r}{\partial u_r}\frac{\partial u_r}{\partial y_j}$

$\frac{\partial u_r}{\partial y_j}=w_{jr}$

$\partial y_j=\sum_{r=1}^{n}\delta_rw_{jr}$

#### 中间层的梯度

##### 相关函数

$y_j=f(u_j)$

$u_j=w_{ij}y_i+b_j$

$y_j$为中间层的输出值

$f$为激励函数

$u_j$输入值与权重的乘积与偏置的和

##### 中间层权重的梯度

$∂ w_{ij}=\frac{∂ E}{∂ w_{ij}}=\frac{∂ E}{∂ uj}\frac{∂{u_j}}{∂ w_{ij}}=\frac{∂ E}{∂ y_j}\frac{∂ y_j}{∂ u_j}\frac{∂ u_j}{∂ w_{ij}}$

$\frac{∂ E}{∂ y_j}$为中间层的输出梯度

$\frac{∂ y_j}{∂ u_j}$为激励函数的微分

$\frac{∂ u_j}{∂ w_{ij}}=y_i$

定义$\delta_j=∂ y_i\frac{∂ y_j}{∂ u_j}$则$∂ w_{ij}=yi\delta_j$

##### 中间层偏置的梯度

$∂ b_j=\frac{∂ E}{∂ b_j}=\frac{∂ E}{∂ uj}\frac{∂{u_j}}{∂ b_j}$

$\frac{∂{u_j}}{∂ b_j}=1$

则$ b_j=\delta_j$

若网络层不止三层，即隐藏层不止三层，则$∂ y_i=\sum_{q=1}^{m}∂_qw_{iq}$

#### 梯度计算公式总结

##### 输出层

$\delta_k=\frac{\partial E}{\partial u_k}=\frac{\partial E}{\partial y_k}\frac{\partial y_k}{\partial u_k}$

$\partial w_{jk} =y_j\delta_k$

$\partial b_k =\delta_k$

$\partial y_j=\sum_{r=1}^{n}\delta_rw_{jr}$

关于$\delta_k$的求解，在使用不同的损失函数和激励函数组合时不同，其方法也是不同的，$\delta_k$与输出层的神经元数量相同

##### 中间层

$\delta_j=\frac{∂ E}{∂ u_j}=∂ y_i\frac{∂ y_j}{∂ u_j}$

$∂ w_{ij}=y_i\delta_j$

$∂ b_j=\delta_j$

$∂ y_i=\sum_{q=1}^{m}\delta_qw_{iq}$

## MLP的建构

```python
import torch
import torch.nn as nn
def self_MLP(nn.module):
  def __init__(self):
    super(self_MLP,self).__init__()
    self.hidden = nn.Sequential(
      nn.Linear(10,128); # 10个输入，第一层隐藏层为10个神经元
      nn.ReLU();         # 激活函数为ReLU()函数 
      nn.Linear(128,64); # 第二个隐藏层为64个神经元
      nn.ReLU();
      nn.Linear(64,32);  # 第三个隐藏层为32个神经元
      nn.ReLU();
    )
  def forward(self,x):
    x = self.hidden(x)
    return x           # 返回x
```