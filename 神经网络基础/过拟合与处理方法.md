# 过拟合预处理方法

## 多层次带来的问题

### 局部最优解的陷阱

* 被局部范围内的最优解束缚
* 某些情况下梯度急剧减少

### 过拟合

Runge现象：多项式拟合时，次数超过七，插值多项式会出现严重的震荡效应。

过拟合：陷入了对特定模式数据进行最优化，而产生的一种局部最优解。

### 梯度消失

进行反向传播时，随着对网络层的反溯，梯度会趋向于零。

例如$sigmoid$函数，梯度最大为0.25，当网络层加多时，追溯梯度会越来越小。

####  解决方法

利用ReLU作为激励函数

$y=\begin{cases}0&\text{x<=0}\\x&\text{x>0}\end{cases}$

确认其微分为：

$y=\begin{cases}0&\text{x<=0}\\1&\text{x>0}\end{cases}$

随着网络层的叠加，梯度不会出现衰退。

### 学习时间过长

## 解决问题的策略

### 超参数的最优化

超参数：神经网络的层数，神经元的个数，学习系数。

### 正则化

正则化是指对权重加以适当的约束。

通过对权重加以限制，防止权重采用极端解而陷入局部最优解。

* 对权重加以限制，通过将加到某个神经元的输入上的权重的平方和收敛。

$\sum_{i}w_{ij}^{2}<c$加在某个神经元的输入上的权重$w_{ij}$的平方和比常量c小。

* 对权重进行衰减处理，可将误差加入权重的平方和，使权重随着误差一起产生衰退。

$E_w=E+\frac{\lambda}{2}\lvert \lvert w\rvert \rvert^{2}$

### 权重与偏差的初始值

随机设置权重和偏置的初始值比较好。使用一定程度上比较小又有细微差别的数值。

* 如果全部使用相同的值对其进行初始化，网络层之间的梯度一样，网络表现能力丧失。
* 权重值设置的太大，容易掉入局部最优解。
* 全部设置为0，导致神经网络丧失表现能力。

### 提前终止

提前停止是值在学习的过程中将其打断的一种方法。

当测试数据的误差出现了多次恶化，确定误差有恶化或停止的倾向，就停止网络学习。

### 数据扩张

如果训练数集的样本很少，出现过学习现象。由于神经网络对非常狭窄的范围的数据进行了最优解，而失去了通用性所导致的。

数据扩张：使用某种方法对样本数据进行加工生成新的样本，以此来增加样本数量。

### 数据的预处理

预处理是指预先将输入数据转换成易于神经网络使用的数据格式的处理方法。

#### 正规化

$x_{i}^{'}=\frac{x_i-min}{max-min}$

```python
import numpy as np
def normalize(x):
    x_max=np.max(x)
    x_min=np.min(x)
    return (x-x_min)/(x_max-x_min)
```

#### 标准化

数据的标准化：通过加工使数据的平均值为0，标准差为1的处理方法。

但是标准化处理并不意味着数据就会变成正态分布。

$x_{i}^{'}=\frac{x_i-\sigma}{\mu}$

```python
import numpy as np

def standardize(x):
    ave=np.average(x)
    std=np.std(x)
    return (x-ave)/std
```

#### 去相关化

当集中的数据之间存在某种关系时，将相关关系剔除的处理被称为对数据的去相关化处理。

#### 白化

对数据进行标准化和去相关化处理后，得到的结果就是被白化的数据。

其平均值是0，离散值是1，各个部分没有任何相关性。

### Dropout

Dropout是指按照一定的概率随机消除输出层以外的神经元的一种处理技巧。

每次对权重和偏置进行跟新所消掉的神经元是不同的。如果神经网络不被消去而保存的概率是p，那么中间层设置为p=0.5，输入层设置为p=0.8~0.9等值。

进行测试时，再将这个p值乘以网络层的输出值，就可以达到与学习时的神经元缩减稍等那部分一致的目的。

本质上讲：将多个彼此不同的神经网络组合在一起进行的学习。

```python
class Dropout:
    def __init__(self,dropout_ratio):
        self.dropout_ratio=dropout_ratio
    
    def forward(self,x,is_train):					#is_train:学习的时候为Ture
        if is_train:
            rand = np.random.rand(*x.shape)			#随机数的矩阵
            self.dropout=np.where(rand>self.dropout_ratio,1,0)
            self.y=x*self.dropout					#随机地将神经元设置为无效zhuang'tai
        else:
            self.y=(1-self.dropout_ratio)*x
            
     def backwards(self,grad_y):
        self.grad_x=grad_y*self.dropout
            
```

一共由m个神经元，在训练时，只有（1-p）m个有效（p为失效的概率），则在学习时，如果全部生效则结果会变大，为使神经网络与训练时结果一致，则在输出结果时，每个神经元输出*（1-p）。

