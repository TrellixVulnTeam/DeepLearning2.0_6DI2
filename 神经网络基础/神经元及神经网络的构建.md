# 神经网络
## 神经细胞的模型化
神经元：多个输入---->权重--->偏置--->激励函数--->输出
符号表示：x为输入函数，W为权重，b为偏置常量，f为激励函数，y为输出
则$y=f（∑（x_i * w_i+b)）$
## 神经元的网络化
在神经网络中，网络层包括`输入层`，`中间层（隐藏层）`，`输出层`。
正向传播：从转递输入信息到产生输出的过程
反向传播：从输出向输入逆向传递信息的过程
符号表示：
前面网络层有m个，后面网络层有n个。则Wij：前面网络层i对后面网络层j的权重，W为m *n的矩阵。
上层网络层有m个神经元，即上层网络的输出向量的元素为m。
i作为上层网络的下标，j作为下层网络的下标。
$Y_i=X_j=(x_1,x_2,x_3...x_m)$ *下层网络层的每一个神经元都有m个输入*
下层神经网络的神经元偏置$B_j=（b_1，b_2,b_3...b_n)$

下层神经网络的输出为$Y_j=(y_1,y_2,y_3...y_n)$

则$Yj=f（\sum(X_j *W_{ij})+B_j)$

## 神经网络的应用
* 回归问题：连续性问题
* 分类问题：离散问题
## 激励函数
### 阶跃函数
```
numpy.where(x<=0,0,1)
```
### sigmoid函数
```
numpy.where(1/1+exp(-x))
```
sigmid的导数：y'=（1-y)*y
### tanh函数
numpy内置tanh函数：tanh（x)=(exp(x)-exp(-x))/(exp(x)+exp(-x))
```
numpy.tanh(x)
```
### ReLU函数
```
numpy.where(x<=0,0,x)
```
### Leaky Relu函数
```
numpy.where(x<=0,0.01*x,x)
```
对于ReLU的优化，对负数区域增加小的梯度避免出现：dying ReLU现象
### 恒等函数
用于输出回归函数的输出层：y=x
### SoftMax函数
$y=exp(X)/\sum exp(X_k)$
```python
numpy.exp(x)/numpy.sum(numpy.exp(x))
```







