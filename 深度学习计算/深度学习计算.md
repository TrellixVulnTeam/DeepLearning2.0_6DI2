# 深度学习计算

深入学习计算的关键组件：

* 模型构建
* 参数访问与初始化
* 设计自定义层和块
* 将模型读写到磁盘
* 利用GPU实现显著的加速

## 层和块

块（block）：描述单个层、由多个层组成的组件或整个模型本⾝。使⽤块进⾏抽象的⼀个好处是可以将⼀些块组合成更⼤的组件，这⼀过程通常是递归的。

从编程的⻆度来看，块由类（class）表⽰。它的任何⼦类都必须定义⼀个将其输⼊转换为输出的前向传播函
数，并且必须存储任何必需的参数。注意，有些块不需要任何参数。最后，为了计算梯度，块必须具有反向
传播函数。

```python
net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))
```

通过实例化nn.Sequential来构建模型，层的执⾏顺序是作为参数传递的。简⽽⾔之， nn.Sequential定义了⼀种特殊的Module，即在PyTorch中表⽰⼀个块的类，它维护了⼀个由Module组成的有序列表。注意，两个全连接层都是Linear类的实例，Linear类本⾝就是Module的⼦类。

### 自定义块

每个块必须提供的基本功能：

1. 将数据作为其前向传播函数的参数
2. 通过前向传播来生成输出。输出的形状可能与输入的形状不同。
3. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。
4. 存储和访问前向传播计算所需的参数。
5. 根据需要初始化模型参数。

```python
class MLP(nn.Module):
	# ⽤模型参数声明层。这⾥，我们声明两个全连接的层
	def __init__(self):
		# 调⽤MLP的⽗类Module的构造函数来执⾏必要的初始化。
		# 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
		super().__init__()
		self.hidden = nn.Linear(20, 256) # 隐藏层
		self.out = nn.Linear(256, 10) # 输出层

    # 定义模型的前向传播，即如何根据输⼊X返回所需的模型输出
	def forward(self, X):
		# 注意，这⾥我们使⽤ReLU的函数版本，其在nn.functional模块中定义。
		return self.out(F.relu(self.hidden(X)))
```

__ init__ 函数通过super().__ init __ 调用父类的__ init__函数，省去了重复编写模板代码的痛苦。

块的⼀个主要优点是它的多功能性。我们可以⼦类化块以创建层（如全连接层的类）、整个模型（如上⾯
的MLP类）或具有中等复杂度的各种组件。

### 顺序块

构建简化的MySequential，实现两个关键函数：

1. 一种将块逐个追加到列表中的函数
2. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。

```python
class MySequential(nn.Module):
  def __init__(self, *args):
    super().__init__()
    for idx,module in enumerate(args):
    	# 这⾥，module是Module⼦类的⼀个实例。我们把它保存在'Module'类的成员
		# 变量_modules中。module的类型是OrderedDict
        self._modules[str(idx)] = module
  
  def forward(self,X):
      # OrderedDict保证了按照成员添加的顺序遍历它们
      for block in self._modules.values():
          X = block(X)
      return X
```

_ _ init_ _  函数将每个模块逐个添加到有序字典_ modules中。_ modules的主要优点是：在模块的参数初始化过程中，系统知道在_ modules字典中查找需要初始化参数的⼦块。

当MySequential的前向传播函数被调⽤时，每个添加的块都按照它们被添加的顺序执⾏。现在可以使⽤我们的MySequential类重新实现多层感知机。

### 前向传播函数中执行代码

网络中所有操作不限于对网络的激活值及网络的参数。有时我们可能希望合并既不是上⼀层的结果也不是可更新参数的项，我们称之为常数参数（constant parameter）。例如，我们需要⼀个计算函数$$f(x;w) = c·w^⊤x$$的层，其中x是输⼊，w是参数，c是某个在优化过程中没有更新的指定常量。

```python
class FixHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        #不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20,20),requires_grad = False)
        self.linear = nn.Linear(20,20)
    
    def forward(self, X):
        X = self.linear(X)
        # 使⽤创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X,self.rand_weight) + 1)
        # 复用全连接层，相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            x /= 2
        return X.sum()
```

它运⾏了⼀个while循环，在L1范数⼤于1的条件下，将输出向量除以2，直到它满⾜条件为⽌。最后，模型返回了X中所有项的和。注意，此操作可能不会常⽤于在任何实际任务中，只是展⽰如何将任意代码集成到神经⽹络计算的流程中。

可以混搭各种组合块的方法，在下面的例子中，以一些想到的方法镶嵌套。

```python
class NextMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),
                                 nn.Linear(64,32),nn.ReLU())
        self.linear = nn.Linear(32,16)
    
    def forward(self, X):
		return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

• ⼀个块可以由许多层组成；⼀个块可以由许多块组成。
• 块可以包含代码。
• 块负责⼤量的内部处理，包括参数初始化和反向传播。
• 层和块的顺序连接由Sequential块处理。

## 参数管理

操作参数：

* 访问参数、用于调试、诊断和可视化
* 参数初始化
* 在不同模型组件间共享参数

```python
net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
X = torch.rand(size=(2,4))
net(X)
```

### 参数访问

我们从已有模型中访问参数。当通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层。这就像模型是⼀个列表⼀样，每层的参数都在其属性中。如下所⽰，为第⼆个全连接层的参数。

```python
print(net[2].state_dict())

"""
OrderedDict([('weight', tensor([[ 0.3231, -0.3373, 0.1639, -0.3125, 0.0527, -0.2957,
, 0.0192, 0.0039]])), ('bias', tensor([-0.2930]))])
"""
```

### 目标参数

要对参数执⾏任何操作，⾸先我们需要访问底层的数值。下⾯的代码从第二个全连接层（即第三个神经⽹络层）提取偏置，提取后返回的是⼀个参数类实例，并进⼀步访问该参数的值。

```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
"""
<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([-0.2930], requires_grad=True)
tensor([-0.2930])
"""
```

参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式参数值的原因。除了值之外，我们还可以访问每个参数的梯度。在上⾯这个⽹络中，由于我们还没有调⽤反向传播，所以参数的梯度处于初始状态。

```python
net[2].weight.grad == True
# True
```

#### 一次性访问所有参数

当我们需要对所有参数执⾏操作时，逐个访问它们可能会很⿇烦。当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，因为我们需要递归整个树来提取每个⼦块的参数。

```python
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
"""
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.
,!Size([1, 8])) ('2.bias', torch.Size([1]))
"""
```

#### 从嵌套块手机参数

```python
def block1():
	return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
						nn.Linear(8, 4), nn.ReLU())
def block2():
	net = nn.Sequential()
	for i in range(4):
		# 在这⾥嵌套
		net.add_module(f'block {i}', block1())
	return net
rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X)
print(rgnet)
"""
Sequential(
(0): Sequential(
	(block 0): Sequential(
	(0): Linear(in_features=4, out_features=8, bias=True)
	(1): ReLU()
	(2): Linear(in_features=8, out_features=4, bias=True)
	(3): ReLU()
 )
 (block 1): Sequential(
	(0): Linear(in_features=4, out_features=8, bias=True)
	(1): ReLU()
	(2): Linear(in_features=8, out_features=4, bias=True)
	(3): ReLU()
 )
(block 2): Sequential(
	(0): Linear(in_features=4, out_features=8, bias=True)
	(1): ReLU()
	(2): Linear(in_features=8, out_features=4, bias=True)
	(3): ReLU()
 )
(block 3): Sequential(
	(0): Linear(in_features=4, out_features=8, bias=True)
	(1): ReLU()
	(2): Linear(in_features=8, out_features=4, bias=True)
	(3): ReLU()
	)
 )
(1): Linear(in_features=4, out_features=1, bias=True)
 )
"""
```

因为层是分层嵌套的，所以也可以像通过嵌套列表索引⼀样访问它们。

### 参数初始化

默认情况下，PyTorch会根据⼀个范围均匀地初始化权重和偏置矩阵，这个范围是根据输⼊和输出维度计算
出的。PyTorch的nn.init模块提供了多种预置初始化⽅法。

#### 内置初始化

调⽤内置的初始化器

```python
# net为 net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
```

我们还可以将所有参数初始化为给定的常数

```python
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight,1)
        nn.init.zeros_(m.bias)
```

对某些块应⽤不同的初始化⽅法

```python
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
       nn.init.constant_(m.weight, 42)

net[0].apply(xavier)
net[2].apply(init_42)
```

自定义初始化

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init",*[(name,param.shape)
                       for name,param in m.named_parameters()][0])
        nn.init.uniform_(m.weight,-10,10)
        m.weight.data *= m.weight.data.abs() >= 5
   
net.apply(my_init)
```

直接设置参数

```python
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
```

### 参数绑定

有时希望在多个层间共享参数：可以定义一个稠密层，然后使用它的参数来设置另一个的参数。

```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8,8)
net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8,1))
net(x)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0,0] = 100
# 确保它们实际上是同一个对象，而不是有相同的值
"""
tensor([True, True, True, True, True, True, True, True])
tensor([True, True, True, True, True, True, True, True])
"""
```

这个例⼦表明第三个和第五个神经⽹络层的参数是绑定的。它们不仅值相等，⽽且由相同的张量表⽰。因此
如果我们改变其中⼀个参数，另⼀个参数也会改变。**由于模型参数包含梯度，因此在反向传播期间第⼆个隐藏层（即第三个神经⽹络层）和第三个隐藏层（即第五个神经⽹络层）的梯度会加在⼀起。**

## 自定义层

### 构建不带参数的层

例子

```python
class CenteredLayer(nn.Module):
    def __init__(slef):
        super().__init__()
    
    def forward(self, X):
        return X - X.mean()
```

### 带参数的层

定义具有参数的层，这些参数可以通过训练进⾏调整。我们可以使⽤内置函数来创建参数，这些函数提供⼀些基本的管理功能。⽐如管理访问、初始化、共享、保存和加载模型参数。这样做的好处之⼀是：我们不需要为每个⾃定义层编写⾃定义的序列化程序。

```python
 # in_units:输入数 units: 输出数
 class MyLinear(nn.Module):
     def __init__(self, in_units, units):
         super().__init__()
         self.weight = nn.Parameter(torch.randn(in_units,units))
         self.bias = nn.Parameter(torch.randn(units,))
     def forward(self, X):
         linear = torch.matual(X , self.weight.data) + self.bias.data
         return F.relu(linear)
```

• 我们可以通过基本层类设计⾃定义层。这允许我们定义灵活的新层，其⾏为与深度学习框架中的任何现有层不同。
• 在⾃定义层定义完成后，我们就可以在任意环境和⽹络架构中调⽤该⾃定义层。
• 层可以有局部参数，这些参数可以通过内置函数创建。

## 读写文件

### 加载和保存张量

```python
x = torch.arrange(4)
torch.save(x,'x-file')  # 保存参数
x2 = torch.load('x-file') # 存储再文件中的数据读回内存
```

### 加载和保存模型参数

深度学习框架提供了内置函数来保存和加载整个⽹络。需要注意的⼀个重要细节是，这将保存模型的参数⽽不是保存整个模型。例如，如果我们有⼀个3层多层感知机，我们需要单独指定架构。因为模型本⾝可以包含任意代码，所以模型本⾝难以序列化。因此，为了恢复模型，我们需要⽤代码⽣成架构，然后从磁盘加载参数。

```python
class MLP(nn.Module):
	def __init__(self):
		super().__init__()
		self.hidden = nn.Linear(20, 256)
		self.output = nn.Linear(256, 10)
	
    def forward(self, x):
		return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)	
```

```python
# 模型参数保存
torch.save(net.state_dict(),'mlp.params')

# 回复模型
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
```

• save和load函数可⽤于张量对象的⽂件读写。
• 我们可以通过参数字典保存和加载⽹络的全部参数。
• 保存架构必须在代码中完成，⽽不是在参数中完成。















